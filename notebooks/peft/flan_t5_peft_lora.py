# -*- coding: utf-8 -*-
"""flan-t5-peft_lora.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UBGTQo70oQtIlDgilmEpNRSpIzdfqo36
"""

# ---- Utility Functions ----


def print_number_of_trainable_model_parameters(model):
    trainable_model_params = 0
    all_model_params = 0
    for _, param in model.named_parameters():
        all_model_params += param.numel()
        if param.requires_grad:
            trainable_model_params += param.numel()
    return (
        f"trainable model parameters: {trainable_model_params}\n"
        f"all model parameters: {all_model_params}\n"
        f"percentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%"
    )


def preprocess_function(examples):
    """Add prefix to the sentences, tokenize the text, and set the labels"""
    prefix = "Give the summary of the article: "
    inputs = [prefix + doc for doc in examples["article"]]
    model_inputs = tokenizer(inputs, max_length=128, truncation=True)
    labels = tokenizer(
        text_target=examples["highlights"], max_length=512, truncation=True
    )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs


def generate_summaries(
    model, tokenizer, inputs, max_input_length=512, max_target_length=128
):
    inputs_tokenized = tokenizer(
        inputs,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=max_input_length,
    ).to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs_tokenized["input_ids"],
            attention_mask=inputs_tokenized["attention_mask"],
            max_length=max_target_length,
        )
    return tokenizer.batch_decode(outputs, skip_special_tokens=True)


def compute_all_summary_metrics(csv_path: str) -> pd.DataFrame:
    import bert_score
    import nltk
    import pandas as pd
    import torch
    from nltk.tokenize import word_tokenize
    from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu
    from rouge_score import rouge_scorer
    from sentence_transformers import SentenceTransformer, util
    from tqdm import tqdm
    from transformers import BartForConditionalGeneration, BartTokenizer

    df = pd.read_csv(csv_path)
    required_columns = ["rlhf_summary", "reference_summary", "article"]
    for col in required_columns:
        if col not in df.columns:
            raise ValueError(f"Missing required column: {col}")

    rouge = rouge_scorer.RougeScorer(["rouge1", "rouge2", "rougeL"], use_stemmer=True)
    tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-cnn")
    model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn").to(
        "cuda" if torch.cuda.is_available() else "cpu"
    )
    fact_model = SentenceTransformer("all-MiniLM-L6-v2")

    def score_fact_consistency_fallback(source, summary):
        emb_source = fact_model.encode(source, convert_to_tensor=True)
        emb_summary = fact_model.encode(summary, convert_to_tensor=True)
        return util.pytorch_cos_sim(emb_source, emb_summary).item()

    def compute_bleu(reference: str, hypothesis: str) -> float:
        reference_tokens = [word_tokenize(reference.lower())]
        hypothesis_tokens = word_tokenize(hypothesis.lower())
        smoothie = SmoothingFunction().method4
        return sentence_bleu(
            reference_tokens,
            hypothesis_tokens,
            weights=(0.25, 0.25, 0.25, 0.25),
            smoothing_function=smoothie,
        )

    results = []
    for _, row in tqdm(df.iterrows(), total=len(df)):
        ref = row["reference_summary"]
        peft = row["rlhf_summary"]
        source_doc = row["article"]

        # ROUGE
        rouge_scores_peft = rouge.score(ref, peft)

        # BERTScore
        _, _, F1_peft = bert_score.score([peft], [ref], lang="en", verbose=False)

        # FactScore (approximate)
        factscore_peft = score_fact_consistency_fallback(source_doc, peft)

        # BLEU
        bleu_peft = compute_bleu(ref, peft)

        results.append(
            {
                "Article": source_doc,
                "Reference Summary": ref,
                "PEFT Summary": peft,
                "ROUGE1_Prec_PEFT": rouge_scores_peft["rouge1"].precision,
                "ROUGE1_Rec_PEFT": rouge_scores_peft["rouge1"].recall,
                "ROUGE1_F1_PEFT": rouge_scores_peft["rouge1"].fmeasure,
                "ROUGE2_Prec_PEFT": rouge_scores_peft["rouge2"].precision,
                "ROUGE2_Rec_PEFT": rouge_scores_peft["rouge2"].recall,
                "ROUGE2_F1_PEFT": rouge_scores_peft["rouge2"].fmeasure,
                "ROUGEL_Prec_Orig": rouge_scores_peft["rougeL"].precision,
                "ROUGEL_Rec_PEFT": rouge_scores_peft["rougeL"].recall,
                "ROUGEL_F1_PEFT": rouge_scores_peft["rougeL"].fmeasure,
                "BERTScore_F1_PEFT": F1_peft.item(),
                "FactScore_PEFT": factscore_peft,
                "BLEU_PEFT": bleu_peft,
            }
        )
    return pd.DataFrame(results)


# ---- Setup and Imports ----

import os
import time

import nltk
import numpy as np
import pandas as pd
import torch
from datasets import load_dataset
from transformers import (AutoModelForSeq2SeqLM, AutoTokenizer,
                          DataCollatorForSeq2Seq, Trainer, TrainingArguments)

from peft import LoraConfig, PeftConfig, PeftModel, TaskType, get_peft_model

os.environ["WANDB_DISABLED"] = "true"
nltk.download("punkt", quiet=True)

# ---- Model and Tokenizer ----

model_name = "google/flan-t5-small"
original_model = AutoModelForSeq2SeqLM.from_pretrained(
    model_name, torch_dtype=torch.bfloat16
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

print(print_number_of_trainable_model_parameters(original_model))

# ---- Dataset Preparation ----

dataset = load_dataset("abisee/cnn_dailymail", "3.0.0")
train_dataset = dataset["train"].select(range(50000)).shuffle(seed=20)
val_dataset = dataset["validation"].select(range(5000)).shuffle(seed=20)
test_dataset = dataset["test"].select(range(2000)).shuffle(seed=20)

train_tokenized_dataset = train_dataset.map(preprocess_function, batched=True)
val_tokenized_dataset = val_dataset.map(preprocess_function, batched=True)
test_tokenized_dataset = test_dataset.map(preprocess_function, batched=True)

# ---- PEFT LoRA Configuration ----

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q", "v"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.SEQ_2_SEQ_LM,
)

peft_model = get_peft_model(original_model, lora_config)
print(print_number_of_trainable_model_parameters(peft_model))

# ---- Training Arguments and Trainer ----

output_dir = f"./peft-dialogue-summary-training-{str(int(time.time()))}"
peft_training_args = TrainingArguments(
    output_dir=output_dir,
    auto_find_batch_size=True,
    learning_rate=1e-3,
    num_train_epochs=3,
    logging_steps=100,
)
data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)

peft_trainer = Trainer(
    model=peft_model,
    args=peft_training_args,
    train_dataset=train_tokenized_dataset,
    eval_dataset=val_tokenized_dataset,
    data_collator=data_collator,
)

# ---- Train the Model ----

peft_trainer.train()

# ---- Save Model and Tokenizer ----

peft_model_path = "./content/drive/MyDrive/peft-flant5"
peft_trainer.model.save_pretrained(peft_model_path)
tokenizer.save_pretrained(peft_model_path)

# ---- Load PEFT Model for Evaluation ----

peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(
    "google/flan-t5-small", torch_dtype=torch.bfloat16
)
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")
peft_model = PeftModel.from_pretrained(
    peft_model_base,
    "./peft-summary-checkpoint-local",
    torch_dtype=torch.bfloat16,
    is_trainable=False,
)
print(print_number_of_trainable_model_parameters(peft_model))

# ---- Generate Summaries ----

test_data = dataset["test"].select(range(2000)).shuffle(seed=12)
articles = test_data["article"]
references = test_data["highlights"]
rlhf_preds = generate_summaries(peft_model, tokenizer, articles)

df = pd.DataFrame(
    {
        "article": articles,
        "reference_summary": references,
        "rlhf_summary": rlhf_preds,
    }
)
print(df.head())
df.to_csv("/content/drive/MyDrive/model_generated_summary-peft-lora.csv", index=False)

# ---- Evaluate Metrics ----

import nltk

nltk.download("punkt", quiet=True)
nltk.download("punkt_tab", quiet=True)

df_metrics = compute_all_summary_metrics(
    "/content/drive/MyDrive/model_generated_summary-peft-lora.csv"
)
df_metrics.to_csv(
    "/content/drive/MyDrive/model_generated_summary_metrics-peft-lora-scores.csv",
    index=False,
)
print(df_metrics)
print(df_metrics.describe())

# ---- Save and Download the PEFT Model ----

import shutil

model_dir = "./peft-summary-checkpoint-local"  # replace with your actual checkpoint
zip_file = "fine-tuned_model_cnn_news_peft_lora"
shutil.make_archive(zip_file, "zip", model_dir)

from google.colab import files

files.download(f"{zip_file}.zip")
